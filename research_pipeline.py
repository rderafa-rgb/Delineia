# -*- coding: utf-8 -*-

import time
import re
from typing import List, Dict, Tuple
import requests
import google.generativeai as genai
import networkx as nx
import matplotlib.pyplot as plt
import os
from dotenv import load_dotenv

# Carregar vari√°veis de ambiente
load_dotenv()

# ==================== CLIENTE OPENALEX ====================
class OpenAlexClient:
    """Cliente para buscar artigos no OpenAlex"""

    def __init__(self, email: str):
        self.email = email
        self.base_url = "https://api.openalex.org/works"

    def normalize_query(self, query: str) -> str:
        """Normaliza query mantendo operadores booleanos"""
        query = query.strip()
        query = re.sub(r'\s+', ' ', query)
        return query

    def search_articles(self, query: str, limit: int = 500) -> List[Dict]:
        """Busca artigos na API do OpenAlex"""
        results = []
        
        print(f"  üîç Buscando: {query[:100]}...")
        
        for page in range(1, 4):
            params = {
                'search': query,
                'per_page': min(200, limit - len(results)),
                'page': page,
                'mailto': self.email
            }
            
            try:
                response = requests.get(self.base_url, params=params, timeout=30)
                
                if response.status_code == 200:
                    data = response.json()
                    works = data.get('results', [])
                    
                    for work in works:
                        doi = work.get('doi', '')
                        if doi and doi.startswith('https://doi.org/'):
                            doi = doi
                        elif doi:
                            doi = f"https://doi.org/{doi}"
                        
                        results.append({
                            'id': work.get('id', ''),
                            'title': work.get('title', ''),
                            'year': work.get('publication_year', ''),
                            'doi': work.get('doi', ''),
                            'url': doi if doi else work.get('id', ''),
                            'concepts': [
                                {'name': c.get('display_name', ''), 
                                 'score': c.get('score', 0),
                                 'level': c.get('level', 0)}
                                for c in work.get('concepts', [])
                            ]
                        })
                    
                    if len(results) >= limit or len(works) < 200:
                        break
                else:
                    print(f"  ‚ö†Ô∏è Erro HTTP {response.status_code}")
                    break
                    
            except Exception as e:
                print(f"  ‚ö†Ô∏è Erro: {str(e)[:50]}")
                break
        
        print(f"  ‚úÖ {len(results)} artigos encontrados")
        return results[:limit]

    def extract_concepts_for_cooccurrence(self, articles: List[Dict],
                                         min_score: float = 0.35,
                                         min_level: int = 0) -> List[List[str]]:
        """Extrai conceitos dos artigos para an√°lise de coocorr√™ncia"""
        concepts_lists = []
        
        for article in articles:
            concepts = [
                c['name']
                for c in article.get('concepts', [])
                if c.get('score', 0) >= min_score and c.get('level', 0) >= min_level
            ]
            
            if concepts:
                concepts_lists.append(concepts)
        
        return concepts_lists


# ==================== GERADOR GEMINI ====================
class GeminiQueryGenerator:
    """
    Gerador de an√°lises usando Gemini AI.
    VERS√ÉO CORRIGIDA COM AJUSTES DO PROF. ELISEO REATEGUI
    """

    def __init__(self):
        try:
            import streamlit as st
            api_key = st.secrets.get("GEMINI_API_KEY", "")
            genai.configure(api_key=api_key)

            self.model = genai.GenerativeModel(
                'gemini-2.5-pro',
                generation_config={
                    'temperature': 1.2,
                    'top_p': 0.95,
                    'top_k': 40,
                    'max_output_tokens': 8192,
                }
            )
            print(f"  ‚úÖ Modelo Gemini inicializado: {self.model.model_name} (modo criativo)")
        except Exception as e:
            print(f"  ‚ö†Ô∏è Erro ao inicializar Gemini: {e}")
            self.model = None

    def _safe_generate(self, prompt: str, fallback: str = "", max_retries: int = 3) -> str:
        """Gera√ß√£o segura com retry e valida√ß√£o flex√≠vel"""
        if not self.model:
            print("  ‚ö†Ô∏è Modelo n√£o dispon√≠vel, usando fallback")
            return fallback

        for attempt in range(max_retries):
            try:
                response = self.model.generate_content(prompt)

                extracted_text = None

                if hasattr(response, 'text'):
                    try:
                        extracted_text = response.text
                    except:
                        pass

                if not extracted_text and hasattr(response, 'candidates'):
                    try:
                        candidate = response.candidates[0]
                        if hasattr(candidate, 'content'):
                            parts = candidate.content.parts
                            if parts and len(parts) > 0:
                                extracted_text = parts[0].text
                    except:
                        pass

                if not extracted_text:
                    extracted_text = str(response)

                if extracted_text:
                    extracted_text = extracted_text.strip()
                    if len(extracted_text) >= 30 and extracted_text != "None":
                        return extracted_text

                if attempt < max_retries - 1:
                    print(f"  ‚è≥ Tentativa {attempt + 2}/{max_retries}...")
                    time.sleep(2)

            except Exception as e:
                print(f"  ‚ö†Ô∏è Tentativa {attempt + 1} falhou: {str(e)[:50]}")
                if attempt < max_retries - 1:
                    time.sleep(3)

        print(f"  ‚ö†Ô∏è Usando fallback ap√≥s {max_retries} tentativas")
        return fallback

    def generate_full_report(self, nome: str, tema: str, questao: str,
                            keywords: List[str]) -> str:
        """
        Gera avalia√ß√£o cr√≠tica e construtiva do projeto
        Prompt EXPL√çCITO sobre quest√£o de pesquisa + orienta√ß√£o ao orientador
        """
        keywords_str = ', '.join(keywords)
        primeiro_nome = nome.split()[0] if nome else "estudante"

        prompt = f"""Voc√™ √© um professor universit√°rio experiente orientando um aluno de pesquisa.

**CONTEXTO DO PROJETO:**

Aluno: {nome} (voc√™ vai cham√°-lo de {primeiro_nome})
Tema proposto: {tema}
Quest√£o de pesquisa: {questao}
Palavras-chave escolhidas: {keywords_str}

---

**SUA TAREFA:**

Escreva DOIS par√°grafos conversando com {primeiro_nome}:

**PAR√ÅGRAFO 1 - Sobre as palavras-chave:**
‚Ä¢ Comece com: "{primeiro_nome}, as palavras-chave que voc√™ designou para o projeto..."
‚Ä¢ Comente especificamente sobre as palavras-chave escolhidas
‚Ä¢ Seja aut√™ntico: se est√£o boas, diga o que est√° bom; se h√° problemas, aponte com clareza mas cuidado
‚Ä¢ Se palavras forem muito amplas, diga quais e por qu√™
‚Ä¢ Se houver redund√¢ncia, mostre
‚Ä¢ Se faltar algo importante, sugira especificamente

**PAR√ÅGRAFO 2 - Sobre a quest√£o de pesquisa:**
‚Ä¢ Comente explicitamente sobre a quest√£o de pesquisa apresentada
‚Ä¢ Analise se est√° clara, vi√°vel e bem delimitada
‚Ä¢ Sugira refinamentos se necess√°rio
‚Ä¢ Relacione com as palavras-chave escolhidas
‚Ä¢ Encerre com: "Recomendo que voc√™ converse com seu orientador sobre esses pontos e observe atentamente o grafo de coocorr√™ncias apresentado adiante, pois ele pode revelar rela√ß√µes importantes entre conceitos que ajudar√£o a refinar suas palavras-chave e a delimitar melhor o escopo da sua pesquisa."

**DIRETRIZES:**
‚Ä¢ Tom de conversa: use "voc√™" e o primeiro nome
‚Ä¢ Honesto mas respeitoso
‚Ä¢ Como um professor que realmente se importa com o aluno
‚Ä¢ N√ÉO use linguagem de parecer formal
‚Ä¢ Seja espec√≠fico sobre ESTAS palavras-chave e ESTA quest√£o
‚Ä¢ N√ÉO use frases gen√©ricas que servem para qualquer projeto
‚Ä¢ Projetos absurdos ou invi√°veis merecem feedback honesto

**IMPORTANTE:** N√ÉO use frases como "Com certeza..." ou express√µes clich√™. Seja direto e genu√≠no.

---

Escreva agora os dois par√°grafos para {primeiro_nome}:"""

        fallback = f"""{primeiro_nome}, as palavras-chave que voc√™ designou para o projeto ({keywords_str}) cobrem alguns aspectos do tema '{tema}'. No entanto, seria importante avaliar se esses termos capturam as nuances espec√≠ficas da sua quest√£o de pesquisa e se h√° necessidade de termos mais espec√≠ficos ou complementares.

Sobre sua quest√£o de pesquisa, '{questao}', √© fundamental verificar se est√° suficientemente delimitada e se oferece um caminho claro para investiga√ß√£o. Recomendo que voc√™ converse com seu orientador sobre esses pontos e observe atentamente o grafo de coocorr√™ncias apresentado adiante, pois ele pode revelar rela√ß√µes importantes entre conceitos que ajudar√£o a refinar suas palavras-chave e a delimitar melhor o escopo da sua pesquisa."""

        return self._safe_generate(prompt, fallback)

    def generate_suggested_keywords(self, nome: str, tema: str, questao: str,
                                   keywords: List[str]) -> str:
        """Sugere palavras-chave complementares em ingl√™s t√©cnico"""
        keywords_str = ', '.join(keywords)

        prompt = f"""Voc√™ √© um bibliotec√°rio especializado em buscas cient√≠ficas.

**PROJETO:**
Tema: {tema}
Quest√£o: {questao}
Palavras atuais do aluno: {keywords_str}

**TAREFA:**
Liste 4-6 termos t√©cnicos EM INGL√äS que sejam:
- Complementares (N√ÉO repetir os que o aluno j√° tem)
- Espec√≠ficos da √°rea de pesquisa
- Reconhecidos na literatura cient√≠fica internacional
- √öteis para ampliar a busca mantendo relev√¢ncia

**INSTRU√á√ïES:**
- Retorne APENAS os termos separados por v√≠rgula
- Sem numera√ß√£o, sem aspas, sem explica√ß√µes
- Apenas: termo1, termo2, termo3, termo4

**EXEMPLO do formato correto:**
cognitive load, metacognition, learning strategies, self-regulation

Gere agora os termos complementares:"""

        fallback = "research methods, empirical studies, theoretical framework, scientific literature"

        result = self._safe_generate(prompt, fallback)
        result = result.replace('\n', ', ').strip()
        return result

    def translate_keywords_to_english(self, keywords: List[str]) -> List[str]:
        """
        Traduz palavras-chave do portugu√™s para ingl√™s.
        M√©todo necess√°rio para compatibilidade com pipeline.
        """
        keywords_str = ', '.join(keywords)

        prompt = f"""Voc√™ √© um tradutor especializado em terminologia cient√≠fica.

**TAREFA:**
Traduza os seguintes termos do PORTUGU√äS para INGL√äS acad√™mico/t√©cnico.

**TERMOS:**
{keywords_str}

**INSTRU√á√ïES:**
- Retorne APENAS os termos traduzidos
- Mesma ordem do original
- Separados por v√≠rgula
- Use terminologia padr√£o em publica√ß√µes cient√≠ficas
- Sem numera√ß√£o, sem explica√ß√µes

**EXEMPLO:**
Entrada: Psicologia, Escola, Professores, Burnout
Sa√≠da: Psychology, School, Teachers, Burnout

**TRADUZA AGORA:**"""

        result = self._safe_generate(prompt, ', '.join(keywords))

        # Limpar e separar
        result = result.replace('\n', ', ')
        result = re.sub(r'[0-9]+\.\s*', '', result)
        translated = [t.strip().strip('"').strip("'") for t in result.split(',') if t.strip()]

        # Se n√£o conseguiu traduzir ou n√∫mero diferente, retornar original
        if len(translated) != len(keywords):
            print(f"  ‚ö†Ô∏è Tradu√ß√£o inconsistente, usando termos originais")
            return keywords

        return translated

    def create_search_string_with_objective(self, tema: str,
                                           original_keywords: List[str],
                                           suggested_keywords: str) -> Tuple[str, str]:
        """Cria string de busca otimizada com l√≥gica booleana"""
        suggested_list = [s.strip() for s in suggested_keywords.split(',') if s.strip()]
        all_keywords = original_keywords + suggested_list

        prompt = f"""Voc√™ √© especialista em recupera√ß√£o de informa√ß√£o cient√≠fica.

**CONTEXTO:**
Tema da pesquisa: {tema}
Termos dispon√≠veis: {', '.join(all_keywords)}

**TAREFA:**
Crie uma string de busca em INGL√äS para bases cient√≠ficas que:

1. **Selecione os melhores termos** (escolha 4-7 termos mais relevantes da lista)
2. **Use operadores booleanos:**
   - AND para termos obrigat√≥rios
   - OR para sin√¥nimos/alternativas (dentro de par√™nteses)
3. **Use aspas** para termos compostos (ex: "mental health")
4. **Agrupe** termos relacionados com par√™nteses
5. **Limite:** m√°ximo 200 caracteres

**DEPOIS:**
Explique em 2-3 linhas o objetivo desta busca.

**FORMATO EXATO DA SA√çDA:**
STRING: (sua string aqui)
OBJETIVO: (explica√ß√£o de 2-3 linhas)

**EXEMPLO:**
STRING: "teacher burnout" AND ("mental health" OR "psychological wellbeing") AND (school OR education)
OBJETIVO: Identificar estudos sobre esgotamento docente relacionados √† sa√∫de mental no contexto escolar, combinando descritores espec√≠ficos do fen√¥meno com termos do ambiente educacional.

**AGORA CRIE PARA O TEMA '{tema}':**"""

        response = self._safe_generate(prompt, "")

        string_match = re.search(r'STRING:\s*(.+?)(?=OBJETIVO:|$)', response, re.DOTALL | re.IGNORECASE)
        obj_match = re.search(r'OBJETIVO:\s*(.+)', response, re.DOTALL | re.IGNORECASE)

        if string_match and obj_match:
            search_str = string_match.group(1).strip()
            search_str = search_str.replace('```', '').replace('\n', ' ')
            search_str = re.sub(r'\s+', ' ', search_str).strip()

            objective = obj_match.group(1).strip()
        else:
            main_terms = ' AND '.join([f'"{k}"' for k in original_keywords[:3]])
            sugg_terms = ' OR '.join([f'"{s}"' for s in suggested_list[:3]])

            if sugg_terms:
                search_str = f"{main_terms} AND ({sugg_terms})"
            else:
                search_str = main_terms

            objective = f"Identificar estudos que investigam {tema}, combinando descritores espec√≠ficos do fen√¥meno com termos t√©cnicos do contexto."

        return search_str, objective

    def create_glossary_and_interpretation(self, concepts: List[str],
                                          tema: str, primeiro_nome: str) -> Tuple[str, str]:
        """Cria gloss√°rio t√©cnico e interpreta√ß√£o detalhada do grafo"""
        if not concepts or len(concepts) < 3:
            return ("Poucos conceitos identificados para an√°lise detalhada.",
                    "Dados insuficientes para interpreta√ß√£o da rede conceitual.")

        concepts = concepts[:9]  # 9 termos (Miller, 7¬±2)
        concepts_list = '\n'.join([f"{i+1}. {c}" for i, c in enumerate(concepts)])

        glossary_prompt = f"""Voc√™ √© um especialista criando um gloss√°rio t√©cnico.

**CONCEITOS IDENTIFICADOS NA REDE BIBLIOM√âTRICA:**
{concepts_list}

**TEMA DO PROJETO:** {tema}

---

**TAREFA:**
Para CADA um dos {len(concepts)} conceitos acima, crie uma entrada de gloss√°rio. Siga do t√≠tulo da se√ß√£o direto ao primeiro conceito do grafo, sem uso de frase intermedi√°ria.

**FORMATO OBRIGAT√ìRIO PARA CADA ENTRADA:**

[N√∫mero]. **[Termo em Ingl√™s]** (Tradu√ß√£o em Portugu√™s) - [Defini√ß√£o t√©cnica de 2-3 linhas]

**REGRAS:**
- Termo em ingl√™s em **negrito**
- Tradu√ß√£o em portugu√™s entre (par√™nteses) SEM negrito
- Tra√ßo " - " ap√≥s os par√™nteses
- Defini√ß√£o clara, t√©cnica e espec√≠fica
- Relacionar com o tema '{tema}' quando poss√≠vel
- Ordem alfab√©tica pelo termo em INGL√äS
- N√ÉO pular nenhum conceito
- PROIBIDO usar frases clich√™ como "Com certeza", "Sem d√∫vida", "√â claro que" ou similares
- Seja direto e t√©cnico

**EXEMPLO DO FORMATO:**
1. **Anxiety** (Ansiedade) - Estado emocional caracterizado por preocupa√ß√£o excessiva, tens√£o e sintomas f√≠sicos de estresse. No contexto de {tema}, este conceito contribui para compreender as dimens√µes psicol√≥gicas do fen√¥meno investigado.

2. **Educational Psychology** (Psicologia Educacional) - Ramo da psicologia que investiga processos de ensino-aprendizagem, desenvolvimento cognitivo e fatores que influenciam o desempenho acad√™mico. Permite an√°lise multifacetada das quest√µes relacionadas a {tema}.

**AGORA CRIE O GLOSS√ÅRIO COMPLETO PARA TODOS OS {len(concepts)} CONCEITOS:**"""

        interpretation_prompt = f"""Voc√™ √© um cientometrista analisando uma rede conceitual.

**CONTEXTO:**
Tema da pesquisa: {tema}
Aluno: {primeiro_nome}

**9 CONCEITOS MAIS CENTRAIS NA REDE (Miller, 7¬±2):**
{concepts_list}

---

**TAREFA:**
Escreva uma interpreta√ß√£o detalhada da rede em 3-4 par√°grafos (m√≠nimo 12 linhas).

**ESTRUTURA:**

**Par√°grafo 1 - Estrutura Geral (3-4 linhas):**
- Quais s√£o os 3-4 conceitos MAIS centrais?
- O que essa centralidade revela sobre o campo?
- Como o conhecimento est√° organizado?

**Par√°grafo 2 - Clusters e Rela√ß√µes (3-4 linhas):**
- Como os conceitos se agrupam?
- H√° subdimens√µes claras no tema?
- Que conex√µes s√£o mais interessantes?

**Par√°grafo 3 - Implica√ß√µes para {primeiro_nome} (4-6 linhas):**
- Como essa estrutura pode orientar o delineamento do escopo?
- H√° lacunas que poderiam ser exploradas?
- H√° oportunidades de pesquisa nas interse√ß√µes?
- Recomenda√ß√µes espec√≠ficas

**TOM:**
- Use "voc√™" e "{primeiro_nome}"
- Cite conceitos espec√≠ficos da rede (n√£o seja gen√©rico)
- Tom anal√≠tico mas acess√≠vel
- Oriente a√ß√µes concretas
- N√ÉO use frases clich√™ como "Com certeza" ou similares

**COMECE COM:**
"{primeiro_nome}, o grafo de coocorr√™ncias revela a estrutura conceitual da literatura sobre {tema}..."

**ESCREVA AGORA A INTERPRETA√á√ÉO COMPLETA:**"""

        print("  üìñ Gerando gloss√°rio...")
        glossary = self._safe_generate(
            glossary_prompt,
            f"Gloss√°rio t√©cnico dos {len(concepts)} conceitos centrais identificados na rede de coocorr√™ncias."
        )

        print("  üìä Gerando interpreta√ß√£o da rede...")
        interpretation = self._safe_generate(
            interpretation_prompt,
            f"{primeiro_nome}, o grafo de coocorr√™ncias revela a estrutura conceitual da literatura sobre {tema}, destacando {', '.join([c for c in concepts[:4]])} como conceitos centrais. A an√°lise desta rede pode orientar o delineamento do escopo da sua pesquisa, identificando √°reas consolidadas e poss√≠veis lacunas para investiga√ß√£o."
        )

        return glossary, interpretation


# ==================== ANALISADOR DE COOCORR√äNCIAS ====================
class CooccurrenceAnalyzer:
    """Analisador de redes"""

    def build_graph(self, concepts_lists: List[List[str]], min_cooc: int = 1) -> nx.Graph:
        """Constr√≥i grafo"""
        G = nx.Graph()

        for concepts in concepts_lists:
            for i, c1 in enumerate(concepts):
                for c2 in concepts[i+1:]:
                    if c1 != c2:
                        if G.has_edge(c1, c2):
                            G[c1][c2]['weight'] += 1
                        else:
                            G.add_edge(c1, c2, weight=1)

        weak_edges = [(u, v) for u, v, d in G.edges(data=True) if d['weight'] < min_cooc]
        G.remove_edges_from(weak_edges)
        G.remove_nodes_from(list(nx.isolates(G)))

        print(f"  üï∏Ô∏è Grafo: {len(G.nodes())} n√≥s, {len(G.edges())} arestas")
        return G

    def get_top_nodes(self, G: nx.Graph, n: int = 9) -> List[str]:
        """
        N√≥s mais centrais
        Default de 9 termos (Miller, 7¬±2)
        """
        if not G.nodes():
            return []

        centrality = nx.degree_centrality(G)
        return [node for node, _ in sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:n]]

    def visualize_graph(self, G: nx.Graph, top_n: int = 9, path: str = 'graph.png') -> str:
        """
        Visualiza√ß√£o
        AJUSTE DO PROF. ELISEO: Default de 9 termos (Miller, 7¬±2)
        """
        top_nodes = self.get_top_nodes(G, top_n)
        if not top_nodes:
            return None

        Gs = G.subgraph(top_nodes).copy()
        pos = nx.spring_layout(Gs, k=0.5, iterations=50, seed=42)

        try:
            from networkx.algorithms import community
            communities = list(community.greedy_modularity_communities(Gs))
            palette = ['#3b82f6', '#ef4444', '#10b981', '#f59e0b', '#8b5cf6', '#ec4899']

            color_map = {}
            for i, comm in enumerate(communities):
                for node in comm:
                    color_map[node] = palette[i % len(palette)]

            colors = [color_map.get(n, '#3b82f6') for n in Gs.nodes()]
        except:
            colors = 'lightblue'

        centrality = nx.degree_centrality(Gs)
        sizes = [centrality[n] * 3000 + 300 for n in Gs.nodes()]

        plt.figure(figsize=(16, 12), facecolor='white')

        nx.draw_networkx_nodes(Gs, pos, node_size=sizes, node_color=colors,
                              alpha=0.85, edgecolors='white', linewidths=2.5)
        nx.draw_networkx_edges(Gs, pos, alpha=0.25, edge_color='gray')
        nx.draw_networkx_labels(Gs, pos, font_size=11, font_weight='bold',
                               font_family='sans-serif')

        plt.title("Rede de Coocorr√™ncia de Conceitos (9 termos - Miller, 7¬±2)", 
                 fontsize=20, fontweight='bold', pad=25)
        plt.axis('off')
        plt.tight_layout()

        plt.savefig(path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()

        print(f"  üé® Visualiza√ß√£o: {path}")
        return path


# ==================== PIPELINE PRINCIPAL ====================
class ResearchScopePipeline:
    """Pipeline completo"""

    def __init__(self, email: str):
        self.openalex = OpenAlexClient(email)
        self.gemini = GeminiQueryGenerator()
        self.analyzer = CooccurrenceAnalyzer()

    def process(self, nome: str, tema: str, questao: str, keywords: List[str]) -> Dict:
        """Executa pipeline completo"""
        print("\n" + "="*80)
        print("üöÄ PIPELINE DELIN√âIA XIV - VERS√ÉO COM AJUSTES DO PROF. ELISEO")
        print("="*80 + "\n")

        primeiro_nome = nome.split()[0] if nome else "estudante"

        # 1. Avalia√ß√£o com 2 par√°grafos (palavras-chave E quest√£o)
        print("üìù Etapa 1/7: Avalia√ß√£o completa (tema+quest√£o+palavras)...")
        full_report = self.gemini.generate_full_report(nome, tema, questao, keywords)

        # 2. Termos complementares
        print("üí° Etapa 2/7: Gerando termos complementares...")
        suggested = self.gemini.generate_suggested_keywords(nome, tema, questao, keywords)
        print(f"     ‚Üí Sugeridos: {suggested[:60]}...")

        # 3. String 100% ingl√™s
        print("üîé Etapa 3/7: Criando string 100% em ingl√™s...")
        search_str, objetivo = self.gemini.create_search_string_with_objective(tema, keywords, suggested)

        # 4. Buscar artigos
        print("üìö Etapa 4/7: Buscando artigos no OpenAlex...")
        articles = self.openalex.search_articles(search_str, 500)

        if len(articles) == 0:
            print("  ‚ö†Ô∏è Sem resultados. Tentando com termos traduzidos...")
            translated = self.gemini.translate_keywords_to_english(keywords)
            alt_search = ' AND '.join([f'"{t}"' for t in translated[:3]])
            articles = self.openalex.search_articles(alt_search, 500)

        # 5. Extrair conceitos
        print("üî¨ Etapa 5/7: Extraindo conceitos...")
        concepts_lists = self.openalex.extract_concepts_for_cooccurrence(articles)

        # 6. Construir grafo
        print("üï∏Ô∏è Etapa 6/7: Construindo grafo...")
        G = self.analyzer.build_graph(concepts_lists, min_cooc=1)

        # 7. Visualizar e interpretar (9 termos - AJUSTE DO PROF. ELISEO)
        print("üé® Etapa 7/7: Gerando visualiza√ß√£o e gloss√°rio (9 termos)...")
        viz_path = self.analyzer.visualize_graph(G, 9)  # Miller, 7¬±2
        top_concepts = self.analyzer.get_top_nodes(G, 9)  # Miller, 7¬±2

        glossary, interpretation = self.gemini.create_glossary_and_interpretation(
            top_concepts, tema, primeiro_nome
        )

        print("\n" + "="*80)
        print("‚úÖ PIPELINE CONCLU√çDO!")
        print("="*80 + "\n")

        return {
            'full_report': full_report,
            'search_string': search_str,
            'search_objective': objetivo,
            'articles_count': len(articles),
            'graph_stats': {'nodes': len(G.nodes()), 'edges': len(G.edges())},
            'visualization_path': viz_path,
            'glossary': glossary,
            'graph_interpretation': interpretation,
            'top_concepts': top_concepts,
            'raw_articles': articles,
            'concepts_lists': concepts_lists,
            'graph': G
        }


# Vari√°vel global necess√°ria
import streamlit as st
OPENALEX_EMAIL = st.secrets.get("OPENALEX_EMAIL", "")