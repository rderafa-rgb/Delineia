# -*- coding: utf-8 -*-

import google.generativeai as genai
import requests
import json
import re
import time
import traceback
from typing import List, Dict, Tuple
import networkx as nx
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

# ==================== CONFIGURA√á√ÉO ====================
import streamlit as st

# Ler credenciais do arquivo secrets.toml
GEMINI_API_KEY = st.secrets["GOOGLE_API_KEY"]
OPENALEX_EMAIL = st.secrets["OPENALEX_EMAIL"]

import google.generativeai as genai
genai.configure(api_key=GEMINI_API_KEY)

# ==================== CLIENTE OPENALEX ====================
class OpenAlexClient:
    """Cliente para buscar artigos cient√≠ficos no OpenAlex"""

    def __init__(self, email: str):
        self.base_url = "https://api.openalex.org/works"
        self.email = email

    def normalize_query(self, query: str) -> str:
        """Normaliza a query de busca"""
        query = re.sub(r'"+', '"', query)
        query = re.sub(r'\s*(AND|OR|NOT)\s*', r' \1 ', query, flags=re.IGNORECASE)
        query = re.sub(r'\s+', ' ', query).strip()
        return query

    def search_articles(self, query: str, limit: int = 500) -> List[Dict]:
        query = self.normalize_query(query)
        results = []
        print("Buscando artigos...")

        for page in range(1, 4):
            try:
                params = {
                    'search': query,
                    'per-page': 200,
                    'page': page,
                    'mailto': self.email,
                    'filter': 'type:article'
                }

                response = requests.get(self.base_url, params=params, timeout=30)
                response.raise_for_status()
                data = response.json()
                page_results = data.get('results', [])

                print(f"Pagina {page}: {len(page_results)} artigos")

                for work in page_results:
                    doi = work.get('doi', '')
                    url = doi if doi else work.get('id', '')

                    results.append({
                        'title': work.get('title', ''),
                        'year': work.get('publication_year'),
                        'doi': doi,
                        'url': url,
                        'concepts': [
                            {
                                'name': c['display_name'],
                                'score': c['score'],
                                'level': c['level']
                            }
                            for c in work.get('concepts', [])
                            if c.get('score', 0) > 0.3
                        ]
                    })

                if len(results) >= limit:
                    break

            except Exception as e:
                print(f"Erro na pagina {page}: {str(e)}")
                break

        print(f"Total: {len(results)} artigos")
        return results[:limit]

    def extract_concepts_for_cooccurrence(self, articles: List[Dict],
                                         min_score: float = 0.35,
                                         min_level: int = 0) -> List[List[str]]:
        """Extrai conceitos dos artigos"""
        concepts_lists = []

        for article in articles:
            concepts = [
                c['name'] for c in article.get('concepts', [])
                if c['score'] >= min_score and c['level'] >= min_level
            ]
            if concepts:
                concepts_lists.append(concepts)

        print(f"  üìä {len(concepts_lists)} artigos com conceitos")
        return concepts_lists


# ==================== GERADOR COM GEMINI ====================
class GeminiQueryGenerator:
    """
    Gerador de an√°lises usando Gemini AI.
    VERS√ÉO CORRIGIDA: Modelo est√°vel + m√©todo de tradu√ß√£o
    """

    def __init__(self):
        try:
            self.model = genai.GenerativeModel(
                'gemini-2.5-pro',  # MODELO EST√ÅVEL
                generation_config={
                    'temperature': 0.95,
                    'top_p': 0.95,
                    'top_k': 40,
                    'max_output_tokens': 8192,
                }
            )
            # FIX: Changed to string concatenation to avoid outer f-string evaluation of self.model
            print("  ‚úÖ Modelo Gemini inicializado: " + self.model.model_name + " (modo criativo)")
        except Exception as e:
            print(f"  ‚ö†Ô∏è Erro ao inicializar Gemini: {e}")
            self.model = None

    def _safe_generate(self, prompt: str, fallback: str = "", max_retries: int = 3) -> str:
        """Gera√ß√£o segura com retry e valida√ß√£o flex√≠vel"""
        if not self.model:
            print("  ‚ö†Ô∏è Modelo n√£o dispon√≠vel, usando fallback")
            return fallback

        for attempt in range(max_retries):
            try:
                response = self.model.generate_content(prompt)

                extracted_text = None

                if hasattr(response, 'text'):
                    try:
                        extracted_text = response.text
                    except:
                        pass

                if not extracted_text and hasattr(response, 'candidates'):
                    try:
                        candidate = response.candidates[0]
                        if hasattr(candidate, 'content'):
                            parts = candidate.content.parts
                            if parts and len(parts) > 0:
                                extracted_text = parts[0].text
                    except:
                        pass

                if not extracted_text:
                    extracted_text = str(response)

                if extracted_text:
                    extracted_text = extracted_text.strip()
                    if len(extracted_text) >= 30 and extracted_text != "None":
                        return extracted_text

                if attempt < max_retries - 1:
                    print(f"  ‚è≥ Tentativa {attempt + 2}/{max_retries}...")
                    time.sleep(2)

            except Exception as e:
                print(f"  ‚ö†Ô∏è Tentativa {attempt + 1} falhou: {str(e)[:50]}")
                if attempt < max_retries - 1:
                    time.sleep(3)

        print(f"  ‚ö†Ô∏è Usando fallback ap√≥s {max_retries} tentativas")
        return fallback

    def generate_full_report(self, nome: str, tema: str, questao: str,
                            keywords: List[str]) -> str:
        """Gera avalia√ß√£o cr√≠tica e construtiva do projeto"""
        keywords_str = ', '.join(keywords)
        primeiro_nome = nome.split()[0] if nome else "estudante"

        prompt = f"""Voc√™ √© um professor universit√°rio experiente orientando um aluno de pesquisa.

**CONTEXTO DO PROJETO:**

Aluno: {nome} (voc√™ vai cham√°-lo de {primeiro_nome})
Tema proposto: {tema}
Quest√£o de pesquisa: {questao}
Palavras-chave escolhidas: {keywords_str}

---

**SUA TAREFA:**

Escreva um par√°grafo conversando com {primeiro_nome} sobre as palavras-chave que ele escolheu.

**DIRETRIZES (use seu julgamento profissional):**

‚Ä¢ Comece com: "{primeiro_nome}, as palavras-chave que voc√™ designou para o projeto..."

‚Ä¢ **Seja aut√™ntico e direto:**
  - Se o projeto fizer sentido, comente o que est√° bom
  - Se houver problemas evidentes (tema absurdo, quest√£o imposs√≠vel, palavras sem rela√ß√£o),
    aponte isso com clareza mas cuidado
  - Se palavras forem muito amplas (ex: "Escola", "Psicologia"), diga quais e por qu√™
  - Se houver redund√¢ncia entre termos, mostre
  - Se faltar algo importante, sugira especificamente o qu√™

‚Ä¢ **Tom de conversa:**
  - Use "voc√™" e o primeiro nome
  - Honesto mas respeitoso
  - Como um professor que realmente se importa com o aluno
  - N√ÉO use linguagem de parecer formal ou formul√°rio padr√£o
  - Pode usar frases como "vejo que...", "considere...", "seria interessante..."

‚Ä¢ **Seja espec√≠fico:**
  - Comente sobre ESTAS palavras-chave espec√≠ficas
  - N√£o use frases gen√©ricas que servem para qualquer projeto
  - Se um termo for bom, diga por qu√™
  - Se um termo for problem√°tico, explique o problema

‚Ä¢ **Encerre com:**
  "Recomendo que voc√™ observe atentamente o grafo de coocorr√™ncias apresentado adiante,
   pois ele pode revelar rela√ß√µes importantes entre conceitos que ajudar√£o a refinar suas
   palavras-chave e a delimitar melhor o escopo da sua pesquisa."

**IMPORTANTE:**
- Projetos com temas claramente absurdos ou quest√µes imposs√≠veis merecem feedback honesto
- N√£o finja que algo invi√°vel √© vi√°vel
- Seja gentil mas n√£o desonesto

---

Escreva agora o par√°grafo para {primeiro_nome}:"""

        fallback = f"""{primeiro_nome}, as palavras-chave que voc√™ designou para o projeto ({keywords_str}) cobrem alguns aspectos do tema '{tema}'. Seria importante conversar com seu orientador para avaliar se esses termos capturam as nuances espec√≠ficas da sua quest√£o de pesquisa e se h√° necessidade de termos mais espec√≠ficos ou complementares. Recomendo que voc√™ observe atentamente o grafo de coocorr√™ncias apresentado adiante, pois ele pode revelar rela√ß√µes importantes entre conceitos que ajudar√£o a refinar suas palavras-chave e a delimitar melhor o escopo da sua pesquisa."""

        return self._safe_generate(prompt, fallback)

    def generate_suggested_keywords(self, nome: str, tema: str, questao: str,
                                   keywords: List[str]) -> str:
        """Sugere palavras-chave complementares em ingl√™s t√©cnico"""
        keywords_str = ', '.join(keywords)

        prompt = f"""Voc√™ √© um bibliotec√°rio especializado em buscas cient√≠ficas.

**PROJETO:**
Tema: {tema}
Quest√£o: {questao}
Palavras atuais do aluno: {keywords_str}

**TAREFA:**
Liste 4-6 termos t√©cnicos EM INGL√äS que sejam:
- Complementares (N√ÉO repetir os que o aluno j√° tem)
- Espec√≠ficos da √°rea de pesquisa
- Reconhecidos na literatura cient√≠fica internacional
- √öteis para ampliar a busca mantendo relev√¢ncia

**INSTRU√á√ïES:**
- Retorne APENAS os termos separados por v√≠rgula
- Sem numera√ß√£o, sem aspas, sem explica√ß√µes
- Apenas: termo1, termo2, termo3, termo4

**EXEMPLO do formato correto:**
cognitive load, metacognition, learning strategies, self-regulation

Gere agora os termos complementares:"""

        fallback = "research methods, empirical studies, theoretical framework, scientific literature"

        result = self._safe_generate(prompt, fallback)
        result = result.replace('\n', ', ').strip()
        return result

    def translate_keywords_to_english(self, keywords: List[str]) -> List[str]:
        """Traduz palavras-chave do portugu√™s para ingl√™s.
        M√©todo necess√°rio para compatibilidade com pipeline.
        """
        keywords_str = ', '.join(keywords)

        prompt = f"""Voc√™ √© um tradutor especializado em terminologia cient√≠fica.

**TAREFA:**
Traduza os seguintes termos do PORTUGU√äS para INGL√äS acad√™mico/t√©cnico.

**TERMOS:**
{keywords_str}

**INSTRU√á√ïES:**
- Retorne APENAS os termos traduzidos
- Mesma ordem do original
- Separados por v√≠rgula
- Use terminologia padr√£o em publica√ß√µes cient√≠ficas
- Sem numera√ß√£o, sem explica√ß√µes

**EXEMPLO:**
Entrada: Psicologia, Escola, Professores, Burnout
Sa√≠da: Psychology, School, Teachers, Burnout

**TRADUZA AGORA:**"""

        result = self._safe_generate(prompt, ', '.join(keywords))

        # Limpar e separar
        result = result.replace('\n', ', ')
        result = re.sub(r'[0-9]+\.\s*', '', result)  # Remove numera√ß√£o
        translated = [t.strip().strip('"').strip("' ") for t in result.split(',') if t.strip()]

        # Se n√£o conseguiu traduzir ou n√∫mero diferente, retornar original
        if len(translated) != len(keywords):
            print(f"  ‚ö†Ô∏è Tradu√ß√£o inconsistente, usando termos originais")
            return keywords

        return translated

    def create_search_string_with_objective(self, tema: str,
                                           original_keywords: List[str],
                                           suggested_keywords: str) -> Tuple[str, str]:
        """Cria string de busca otimizada com l√≥gica booleana"""
        suggested_list = [s.strip() for s in suggested_keywords.split(',') if s.strip()]
        all_keywords = original_keywords + suggested_list

        prompt = f"""Voc√™ √© especialista em recupera√ß√£o de informa√ß√£o cient√≠fica.

**CONTEXTO:**
Tema da pesquisa: {tema}
Termos dispon√≠veis: {', '.join(all_keywords)}

**TAREFA:**
Crie uma string de busca em INGL√äS para bases cient√≠ficas que:

1. **Selecione os melhores termos** (escolha 4-7 termos mais relevantes da lista)
2. **Use operadores booleanos:**
   - AND para termos obrigat√≥rios
   - OR para sin√¥nimos/alternativas (dentro de par√™nteses)
3. **Use aspas** para termos compostos (ex: "mental health")
4. **Agrupe** termos relacionados com par√™nteses
5. **Limite:** m√°ximo 200 caracteres

**DEPOIS:**
Explique em 2-3 linhas o objetivo desta busca.

**FORMATO EXATO DA SA√çDA:**
STRING: (sua string aqui)
OBJETIVO: (explica√ß√£o de 2-3 linhas)

**EXEMPLO:**
STRING: "teacher burnout" AND ("mental health" OR "psychological wellbeing") AND (school OR education)
OBJETIVO: Identificar estudos sobre esgotamento docente relacionados √† sa√∫de mental no contexto escolar, combinando descritores espec√≠ficos do fen√¥meno com termos do ambiente educacional.

**AGORA CRIE PARA O TEMA '{tema}':**"""

        response = self._safe_generate(prompt, "")

        string_match = re.search(r'STRING:\s*(.+?)(?=OBJETIVO:|$)', response, re.DOTALL | re.IGNORECASE)
        obj_match = re.search(r'OBJETIVO:\s*(.+)', response, re.DOTALL | re.IGNORECASE)

        if string_match and obj_match:
            search_str = string_match.group(1).strip()
            search_str = search_str.replace('```', '').replace('\n', ' ')
            search_str = re.sub(r'\s+', ' ', search_str).strip()

            objective = obj_match.group(1).strip()
        else:
            main_terms = ' AND '.join([f'"{k}"' for k in original_keywords[:3]])
            sugg_terms = ' OR '.join([f'"{s}"' for s in suggested_list[:3]])

            if sugg_terms:
                search_str = f"{main_terms} AND ({sugg_terms})"
            else:
                search_str = main_terms

            objective = f"Identificar estudos que investigam {tema}, combinando descritores espec√≠ficos do fen√¥meno com termos t√©cnicos do contexto."

        return search_str, objective

    def create_glossary_and_interpretation(self, concepts: List[str],
                                          tema: str, primeiro_nome: str) -> Tuple[str, str]:
        """Cria gloss√°rio t√©cnico e interpreta√ß√£o detalhada do grafo"""
        if not concepts or len(concepts) < 3:
            return ("Poucos conceitos identificados para an√°lise detalhada.",
                    "Dados insuficientes para interpreta√ß√£o da rede conceitual.")

        concepts = concepts[:15]
        concepts_list = '\n'.join([f"{i+1}. {c}" for i, c in enumerate(concepts)])

        glossary_prompt = f"""Voc√™ √© um especialista criando um gloss√°rio t√©cnico.

**CONCEITOS IDENTIFICADOS NA REDE BIBLIOM√âTRICA:**
{concepts_list}

**TEMA DO PROJETO:** {tema}

---

**TAREFA:**
Para CADA um dos {len(concepts)} conceitos acima, crie uma entrada de gloss√°rio.

**FORMATO OBRIGAT√ìRIO PARA CADA ENTRADA:**

[N√∫mero]. **[Termo em Ingl√™s]** (Tradu√ß√£o em Portugu√™s) - [Defini√ß√£o t√©cnica de 2-3 linhas]

**REGRAS:**
- Termo em ingl√™s em **negrito**
- Tradu√ß√£o em portugu√™s entre (par√™nteses) SEM negrito
- Tra√ßo " - " ap√≥s os par√™nteses
- Defini√ß√£o clara, t√©cnica e espec√≠fica
- Relacionar com o tema '{tema}' quando poss√≠vel
- Ordem alfab√©tica pelo termo em INGL√äS
- N√ÉO pular nenhum conceito

**EXEMPLO DO FORMATO:**
1. **Anxiety** (Ansiedade) - Estado emocional caracterizado por preocupa√ß√£o excessiva, tens√£o e sintomas f√≠sicos de estresse. No contexto de {tema}, este conceito contribui para compreender as dimens√µes psicol√≥gicas do fen√¥meno investigado.

2. **Educational Psychology** (Psicologia Educacional) - Ramo da psicologia que investiga processos de ensino-aprendizagem, desenvolvimento cognitivo e fatores que influenciam o desempenho acad√™mico. Permite an√°lise multifacetada das quest√µes relacionadas a {tema}.

**AGORA CRIE O GLOSS√ÅRIO COMPLETO PARA TODOS OS {len(concepts)} CONCEITOS:**"""

        interpretation_prompt = f"""Voc√™ √© um cientometrista analisando uma rede conceitual.

**CONTEXTO:**
Tema da pesquisa: {tema}
Aluno: {primeiro_nome}

**15 CONCEITOS MAIS CENTRAIS NA REDE:**
{concepts_list}

---

**TAREFA:**
Escreva uma interpreta√ß√£o detalhada da rede em 3-4 par√°grafos (m√≠nimo 12 linhas).

**ESTRUTURA:**

**Par√°grafo 1 - Estrutura Geral (3-4 linhas):**
- Quais s√£o os 3-4 conceitos MAIS centrais?
- O que essa centralidade revela sobre o campo?
- Como o conhecimento est√° organizado?

**Par√°grafo 2 - Clusters e Rela√ß√µes (3-4 linhas):**
- Como os conceitos se agrupam?
- H√° subdimens√µes claras no tema?
- Que conex√µes s√£o mais interessantes?

**Par√°grafo 3 - Implica√ß√µes para {primeiro_nome} (4-6 linhas):**
- Como essa estrutura pode orientar o delineamento do escopo?
- H√° lacunas que poderiam ser exploradas?
- H√° oportunidades de pesquisa nas intersec√ß√µes?
- Recomenda√ß√µes espec√≠ficas

**TOM:**
- Use "voc√™" e "{primeiro_nome}"
- Cite conceitos espec√≠ficos da rede (n√£o seja gen√©rico)
- Tom anal√≠tico mas acess√≠vel
- Oriente a√ß√µes concretas

**COMECE COM:**
"{primeiro_nome}, o grafo de coocorr√™ncias revela a estrutura conceitual da literatura sobre {tema}..."

**ESCREVA AGORA A INTERPRETA√á√ÉO COMPLETA:**"""

        print("  üî§ Gerando gloss√°rio...")
        glossary = self._safe_generate(
            glossary_prompt,
            f"Gloss√°rio t√©cnico dos {len(concepts)} conceitos centrais identificados na rede de coocorr√™ncias."
        )

        print("  üìä Gerando interpreta√ß√£o da rede...")
        interpretation = self._safe_generate(
            interpretation_prompt,
            f"{primeiro_nome}, o grafo de coocorr√™ncias revela a estrutura conceitual da literatura sobre {tema}, destacando {', '.join([c for c in concepts[:4]])} como conceitos centrais. A an√°lise desta rede pode orientar o delineamento do escopo da sua pesquisa, identificando √°reas consolidadas e poss√≠veis lacunas para investiga√ß√£o."
        )

        return glossary, interpretation


# ==================== ANALISADOR DE COOCORR√äNCIAS ====================
class CooccurrenceAnalyzer:
    """Analisador de redes"""

    def build_graph(self, concepts_lists: List[List[str]], min_cooc: int = 1) -> nx.Graph:
        """Constr√≥i grafo"""
        G = nx.Graph()

        for concepts in concepts_lists:
            for i, c1 in enumerate(concepts):
                for c2 in concepts[i+1:]:
                    if c1 != c2:
                        if G.has_edge(c1, c2):
                            G[c1][c2]['weight'] += 1
                        else:
                            G.add_edge(c1, c2, weight=1)

        weak_edges = [(u, v) for u, v, d in G.edges(data=True) if d['weight'] < min_cooc]
        G.remove_edges_from(weak_edges)
        G.remove_nodes_from(list(nx.isolates(G)))

        print(f"  üï∏Ô∏è Grafo: {len(G.nodes())} n√≥s, {len(G.edges())} arestas")
        return G

    def get_top_nodes(self, G: nx.Graph, n: int = 15) -> List[str]:
        """N√≥s mais centrais"""
        if not G.nodes():
            return []

        centrality = nx.degree_centrality(G)
        return [node for node, _ in sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:n]]

    def visualize_graph(self, G: nx.Graph, top_n: int = 15, path: str = 'graph.png') -> str:
        """Visualiza√ß√£o"""
        top_nodes = self.get_top_nodes(G, top_n)
        if not top_nodes:
            return None

        Gs = G.subgraph(top_nodes).copy()
        pos = nx.spring_layout(Gs, k=0.5, iterations=50, seed=42)

        try:
            from networkx.algorithms import community
            communities = list(community.greedy_modularity_communities(Gs))
            palette = ['#3b82f6', '#ef4444', '#10b981', '#f59e0b', '#8b5cf6', '#ec4899']

            color_map = {}
            for i, comm in enumerate(communities):
                for node in comm:
                    color_map[node] = palette[i % len(palette)]

            colors = [color_map.get(n, '#3b82f6') for n in Gs.nodes()]
        except:
            colors = 'lightblue'

        centrality = nx.degree_centrality(Gs)
        sizes = [centrality[n] * 3000 + 300 for n in Gs.nodes()]

        plt.figure(figsize=(16, 12), facecolor='white')

        nx.draw_networkx_nodes(Gs, pos, node_size=sizes, node_color=colors,
                              alpha=0.85, edgecolors='white', linewidths=2.5)
        nx.draw_networkx_edges(Gs, pos, alpha=0.25, edge_color='gray')
        nx.draw_networkx_labels(Gs, pos, font_size=11, font_weight='bold',
                               font_family='sans-serif')

        plt.title("Rede de Coocorr√™ncia de Conceitos", fontsize=20,
                 fontweight='bold', pad=25)
        plt.axis('off')
        plt.tight_layout()

        plt.savefig(path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()

        print(f"  üé® Visualiza√ß√£o: {path}")
        return path


# ==================== PIPELINE PRINCIPAL ====================
class ResearchScopePipeline:
    """Pipeline completo"""

    def __init__(self, email: str):
        self.openalex = OpenAlexClient(email)
        self.gemini = GeminiQueryGenerator()
        self.analyzer = CooccurrenceAnalyzer()

    def process(self, nome: str, tema: str, questao: str, keywords: List[str]) -> Dict:
        """Executa pipeline completo"""
        print("\n" + "="*80)
        print("üöÄ PIPELINE FINAL - VERS√ÉO DEFINITIVA")
        print("="*80 + "\n")

        primeiro_nome = nome.split()[0] if nome else "estudante"

        # 1. Avalia√ß√£o com 2 par√°grafos
        print("üìù Etapa 1/7: Avalia√ß√£o completa (tema+quest√£o+palavras)...")
        full_report = self.gemini.generate_full_report(nome, tema, questao, keywords)

        # 2. Termos complementares
        print("üí° Etapa 2/7: Gerando termos complementares...")
        suggested = self.gemini.generate_suggested_keywords(nome, tema, questao, keywords)
        print(f"     ‚Üí Sugeridos: {suggested[:60]}...")

        # 3. String 100% ingl√™s
        print("üîé Etapa 3/7: Criando string 100% em ingl√™s...")
        search_str, objetivo = self.gemini.create_search_string_with_objective(tema, keywords, suggested)

        # 4. Buscar artigos
        print("üìö Etapa 4/7: Buscando artigos no OpenAlex...")
        articles = self.openalex.search_articles(search_str, 500)

        if len(articles) == 0:
            print("  ‚ö†Ô∏è Sem resultados. Tentando com termos traduzidos...")
            translated = self.gemini.translate_keywords_to_english(keywords)
            alt_search = ' AND '.join([f'"{t}"' for t in translated[:3]])
            articles = self.openalex.search_articles(alt_search, 500)

        # 5. Extrair conceitos
        print("üî¨ Etapa 5/7: Extraindo conceitos...")
        concepts_lists = self.openalex.extract_concepts_for_cooccurrence(articles)

        # 6. Construir grafo
        print("üï∏Ô∏è Etapa 6/7: Construindo grafo...")
        G = self.analyzer.build_graph(concepts_lists, min_cooc=1)

        # 7. Visualizar e interpretar
        print("üé® Etapa 7/7: Gerando visualiza√ß√£o e gloss√°rio...")
        viz_path = self.analyzer.visualize_graph(G, 15)
        top_concepts = self.analyzer.get_top_nodes(G, 15)

        glossary, interpretation = self.gemini.create_glossary_and_interpretation(
            top_concepts, tema, primeiro_nome
        )

        print("\n" + "="*80)
        print("‚úÖ PIPELINE CONCLU√çDO!")
        print("="*80 + "\n")

        return {
            'full_report': full_report,
            'search_string': search_str,
            'search_objective': objetivo,
            'articles_count': len(articles),
            'graph_stats': {'nodes': len(G.nodes()), 'edges': len(G.edges())},
            'visualization_path': viz_path,
            'glossary': glossary,
            'graph_interpretation': interpretation,
            'top_concepts': top_concepts,
            'raw_articles': articles,
            'concepts_lists': concepts_lists,
            'graph': G
        }
